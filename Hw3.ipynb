{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0c515e-ea0a-4fb9-a17d-93244c069716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6100580-ce91-4a22-b9a2-f32db54e2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e88cbb-48e9-4e9c-8172-7c2aa7ae4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вибір ознак (X) та цільової змінної (y)\n",
    "features = ['area', 'bathrooms', 'bedrooms']\n",
    "X_data = df[features].values\n",
    "y = df['price'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534293ee-04d0-4d4f-babd-750e7708839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормалізація ознак \n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73bdebe6-15b6-4710-b85e-b7b93c01cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Додавання стовпця одиниць для зміщення w(0)\n",
    "X_b = np.c_[np.ones((len(X_norm), 1)), X_norm]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e8200-110c-42a5-9a1f-eae0cfba8763",
   "metadata": {},
   "source": [
    "### Функція Гіпотези (Прогноз)\n",
    "\n",
    "$$h_w(X) = Xw$$\n",
    "\n",
    "Де:\n",
    "*   $h_w(X)$ — вектор прогнозованих значень.\n",
    "*   $X$ — матриця ознак (з доданим стовпцем одиниць для коефіцієнта зсуву/перехоплення).\n",
    "*   $w$ (або $\\theta$ / $\\beta$) — вектор коефіцієнтів (ваг) моделі, який ми намагаємося навчити."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184e639e-1405-4cf8-a275-fd249a80b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis(X, w):\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f2b1c-8c68-479c-9f7d-99ee6b27b090",
   "metadata": {},
   "source": [
    "### Функція Втрат (Mean Squared Error - MSE)\n",
    "\n",
    "В матричній формі MSE записується як:\n",
    "\n",
    "$$J(w) = \\frac{1}{2m} (Xw - y)^T (Xw - y)$$\n",
    "\n",
    "Де:\n",
    "*   $J(w)$ — значення функції втрат для поточного набору коефіцієнтів $w$.\n",
    "*   $X$ — матриця ознак (з доданим стовпцем одиниць).\n",
    "*   $w$ — вектор коефіцієнтів (ваг) моделі.\n",
    "*   $y$ — вектор фактичних значень цільової змінної.\n",
    "*   $m$ — кількість навчальних прикладів (кількість рядків у $X$ та $y$).\n",
    "*   $T$ — операція транспонування матриці."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aa7085f-2e9c-4f98-9cff-915f5bdc73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, w):\n",
    "    m = len(y)\n",
    "    error = hypothesis(X, w) - y\n",
    "    J = (1/(2*m)) * (error.T @ error)\n",
    "    return J[0, 0] # Повертаємо скалярне значення"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3a0d2-2389-44ec-b2ad-e3914b1d20a9",
   "metadata": {},
   "source": [
    "### Крок Градієнтного Спуску\n",
    "\n",
    "**Градієнтний спуск** — це ітераційний оптимізаційний алгоритм, який використовується для пошуку мінімуму функції втрат (Cost Function), наприклад, функції $J(w)$ (MSE). Він працює, поступово коригуючи вектор коефіцієнтів $w$ у напрямку, протилежному градієнту функції втрат.\n",
    "\n",
    "$$w_{\\text{новий}} = w_{\\text{старий}} - \\alpha \\cdot \\frac{1}{m} X^T (Xw - y)$$\n",
    "\n",
    "Де:\n",
    "*   $w_{\\text{новий}}$ — оновлений вектор коефіцієнтів після одного кроку.\n",
    "*   $w_{\\text{старий}}$ — поточний вектор коефіцієнтів.\n",
    "*   $\\alpha$ (альфа) — **швидкість навчання (learning rate)**. Це позитивне скалярне значення, яке контролює розмір кожного кроку, який робить алгоритм у напрямку мінімуму. Занадто велика $\\alpha$ може призвести до \"перескакування\" мінімуму, занадто мала — до повільної збіжності.\n",
    "*   $\\frac{1}{m}$ — множник, де $m$ — кількість навчальних прикладів. Це забезпечує, що розмір кроку не залежить від кількості даних.\n",
    "*   $X^T$ — транспонована матриця ознак $X$. Матриця $X$ має форму $(m, n+1)$, де $n+1$ — кількість ознак (включаючи стовпець одиниць). Отже, $X^T$ має форму $(n+1, m)$.\n",
    "*   $(Xw - y)$ — вектор помилок (різниць між прогнозованими та фактичними значеннями). $Xw$ — це $h_w(X)$ (прогнози моделі), тому $(Xw - y)$ — це вектор $(h_w(X) - y)$. Його форма $(m, 1)$.\n",
    "\n",
    "**Пояснення обчислення градієнта ($\\frac{1}{m} X^T (Xw - y)$):**\n",
    "\n",
    "Вираз $\\frac{1}{m} X^T (Xw - y)$ є **градієнтом функції втрат $J(w)$** щодо вектора коефіцієнтів $w$. Градієнт вказує на напрямок найшвидшого зростання функції. Оскільки ми хочемо *мінімізувати* функцію втрат, ми рухаємося у *протилежному* напрямку (звідси знак мінус у формулі).\n",
    "\n",
    "*   Множення $X^T$ (форма $(n+1, m)$) на $(Xw - y)$ (форма $(m, 1)$) дає вектор форми $(n+1, 1)$. Цей вектор має стільки ж елементів, скільки й $w$, і кожен елемент вказує, наскільки потрібно змінити відповідний коефіцієнт $w_i$.\n",
    "*   Ділення на $m$ усереднює градієнт по всіх навчальних прикладах.\n",
    "\n",
    "Таким чином, на кожному кроці градієнтного спуску ми обчислюємо, наскільки сильно ми \"промахнулися\" з прогнозами, а потім коригуємо коефіцієнти $w$ у напрямку, який, як очікується, зменшить ці помилки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f2511f-9fc8-4869-97c6-6cca6f551c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(X, y, w, alpha):\n",
    "    m = len(y)\n",
    "    error = hypothesis(X, w) - y\n",
    "    gradient = (1/m) * (X.T @ error)\n",
    "    w_new = w - alpha * gradient\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df63a4e-fa44-4dcd-b14d-0cd45438436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Навчимо модель, використовуючи функцію градієнтного спуску\n",
    "alpha = 0.01  # Швидкість навчання\n",
    "iterations = 2000 # Кількість кроків\n",
    "w_gs = np.zeros((X_b.shape[1], 1)) # Ініціалізація ваг нулями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f25e89ef-c2c0-4863-b5ec-9e5d8431158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Градієнтний Спуск ---\n",
      "Фінальна функція втрат (MSE/2): 895585024988.66\n",
      "Оптимальні ваги (w0, w1, w2, w3) для нормалізованих даних:\n",
      "[4766729.24770638  821214.14349549  695808.52272316  299983.5710817 ]\n"
     ]
    }
   ],
   "source": [
    "# Цикл навчання\n",
    "for i in range(iterations):\n",
    "    w_gs = gradient_descent_step(X_b, y, w_gs, alpha)\n",
    "    \n",
    "final_cost = cost_function(X_b, y, w_gs)\n",
    "\n",
    "print(\"--- Градієнтний Спуск ---\")\n",
    "print(f\"Фінальна функція втрат (MSE/2): {final_cost:.2f}\")\n",
    "print(\"Оптимальні ваги (w0, w1, w2, w3) для нормалізованих даних:\")\n",
    "print(w_gs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1396ae0-2cd5-4314-b66c-ab4a6aa45dcb",
   "metadata": {},
   "source": [
    "### Аналітичне Рішення (Ordinary Least Squares - OLS)\n",
    "\n",
    "**Аналітичне Рішення**, або **нормальне рівняння**, є прямим методом для знаходження оптимальних коефіцієнтів $w$ для лінійної регресії, мінімізуючи функцію втрат (MSE) без ітерацій.\n",
    "\n",
    "Формула для обчислення вектора коефіцієнтів $w_{OLS}$ за допомогою нормального рівняння:\n",
    "\n",
    "$$w_{\\text{OLS}} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Де:\n",
    "*   $w_{\\text{OLS}}$ — вектор оптимальних коефіцієнтів, знайдений методом найменших квадратів.\n",
    "*   $X$ — матриця ознак (з доданим стовпцем одиниць). Для OLS часто використовують необроблені дані, без нормалізації, але це не є строгим правилом.\n",
    "*   $y$ — вектор фактичних значень цільової змінної.\n",
    "*   $X^T$ — транспонована матриця ознак.\n",
    "*   $(...)^{-1}$ — операція обчислення **оберненої матриці**. Ця операція є центральною для аналітичного вирішення і дозволяє безпосередньо знайти $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "958ff555-683b-458b-a766-b4336ed396a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Аналітичне Рішення (OLS) ---\n",
      "Оптимальні ваги (w0, w1, w2, w3) для ненормалізованих даних:\n",
      "[-1.73171608e+05  3.78762754e+02  1.38604950e+06  4.06820034e+05]\n"
     ]
    }
   ],
   "source": [
    "# Створення матриці з ненормалізованими ознаками + стовпець одиниць\n",
    "X_ols = np.c_[np.ones((len(X_data), 1)), X_data]\n",
    "# Обчислення Normal Equation\n",
    "# Використовуємо np.linalg.pinv (псевдообернена матриця) для кращої стабільності\n",
    "w_ols = np.linalg.pinv(X_ols.T @ X_ols) @ X_ols.T @ y\n",
    "\n",
    "print(\"--- Аналітичне Рішення (OLS) ---\")\n",
    "print(\"Оптимальні ваги (w0, w1, w2, w3) для ненормалізованих даних:\")\n",
    "print(w_ols.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "969f7ccf-86ed-499b-89e3-6e066ef20140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Перевірка за допомогою scikit-learn ---\n",
      "Зміщення (w0 - Intercept): -173171.60763263796\n",
      "Ваги (w1, w2, w3 - Coefficients): [3.78762754e+02 1.38604950e+06 4.06820034e+05]\n",
      "\n",
      "--- Порівняння (OLS vs Sklearn) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Параметр</th>\n",
       "      <th>OLS (Аналітичний)</th>\n",
       "      <th>Sklearn (Library)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w0 (Зміщення)</td>\n",
       "      <td>-173171.61</td>\n",
       "      <td>-173171.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w1 (area)</td>\n",
       "      <td>378.76</td>\n",
       "      <td>378.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w2 (bathrooms)</td>\n",
       "      <td>1386049.50</td>\n",
       "      <td>1386049.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w3 (bedrooms)</td>\n",
       "      <td>406820.03</td>\n",
       "      <td>406820.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Параметр  OLS (Аналітичний)  Sklearn (Library)\n",
       "0   w0 (Зміщення)         -173171.61         -173171.61\n",
       "1       w1 (area)             378.76             378.76\n",
       "2  w2 (bathrooms)         1386049.50         1386049.50\n",
       "3   w3 (bedrooms)          406820.03          406820.03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Створення та навчання моделі на НЕНОРМАЛІЗОВАНИХ даних\n",
    "sklearn_reg = LinearRegression()\n",
    "sklearn_reg.fit(X_data, y)\n",
    "\n",
    "# Виведення результатів\n",
    "print(\"\\n--- 5. Перевірка за допомогою scikit-learn ---\")\n",
    "print(\"Зміщення (w0 - Intercept):\", sklearn_reg.intercept_[0])\n",
    "print(\"Ваги (w1, w2, w3 - Coefficients):\", sklearn_reg.coef_[0])\n",
    "\n",
    "# Порівняння OLS та Sklearn (повинні бути ідентичними)\n",
    "w_sklearn_combined = np.r_[sklearn_reg.intercept_, sklearn_reg.coef_[0]]\n",
    "\n",
    "print(\"\\n--- Порівняння (OLS vs Sklearn) ---\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Параметр': ['w0 (Зміщення)', 'w1 (area)', 'w2 (bathrooms)', 'w3 (bedrooms)'],\n",
    "    'OLS (Аналітичний)': w_ols.flatten(),\n",
    "    'Sklearn (Library)': w_sklearn_combined\n",
    "})\n",
    "display(comparison_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e296e31-aa12-421d-bca1-7fabe6ab4edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
